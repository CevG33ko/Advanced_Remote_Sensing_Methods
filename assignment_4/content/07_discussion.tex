\chapter{Discussion}
\section{Problem 1: Simple network}
The concatenation of the results of the define propagation function is very similar to the Pytorch setup of a neural network with the biggest difference being the extra efficiency provided by the Pytorch implementation of the tensor operations with possible access to a GPU if available on the system.
\section{Problem 2: Backpropagation}
When comparing the two error functions given for the problem it becomes evident that the minimum of the nonlinear equation b) is more suitable to fit the model to the data than the linear function a).
\section{Problem 3: Artificial neural network}
As can be seen in the number of neurons of the network it appears that the input features are strongly correlated since they can only be accurately classified by a layer with a number of neurons which is only a fraction of the number of input features.
\section{Problem 4: Gradient Descent}
As is can seen in \ref{problem4_result} the result is very close to the actuall given line and fits very well.
Over all nearly 1000 iterations are calculated but the first 150 iterations have the most impact to the loss.
All further iterations still minimize the error but only with small effects.

These results are indicators for a very well fitting model to calculate the gradient and weights.
