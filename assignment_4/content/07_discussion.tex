\chapter{Discussion}
\section{Problem 1: Simple network}
The concatenation of the results of the define propagation function is very similar to the Pytorch setup of a neural network with the biggest difference being the extra efficiency provided by the Pytorch implementation of the tensor operations with possible access to a GPU if available on the system.
\section{Problem 2: Backpropagation}
When comparing the two error functions given for the problem it becomes evident that the minimum of the nonlinear equation b) is more suitable to fit the model to the data than the linear function a).
\section{Problem 3: Artificial neural network}
As can be seen in the number of neurons of the network it appears that the input features are strongly correlated since they can only be accurately classified by a layer with a number of neurons which is only a fraction of the number of input features. Pruning of the input features could result in better results for the model.
\section{Problem 4: Gradient Descent}
As is can seen in \ref{problem4_result} the result is very close to the actuall given line and fits very well.
Over all nearly 1000 iterations are calculated but after the first 160 iterations the result is already near the true values. 
This can also be seen in the loss curve.
All further iterations still minimize the error but only with small effects.

These results are indicators for a very well fitting model to calculate the gradient and weights.
