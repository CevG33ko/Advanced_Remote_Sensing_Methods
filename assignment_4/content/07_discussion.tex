\chapter{Discussion}
\section{Problem 1: Simple network}
The concatenation of the results of the define propagation function is very similar to the Pytorch setup of a neural network with the biggest difference being the extra efficiency provided by the Pytorch implementation of the tensor operations with possible access to a GPU if available on the system.
\section{Problem 2: Backpropagation}
When comparing the two error functions given for the problem it becomes evident that the minimum of the nonlinear equation b) is more suitable to fit the model to the data than the linear function a).
\section{Problem 3: Artificial neural network}
As can be seen in the number of neurons of the network it appears that the input features are strongly correlated since they can only be accurately classified by a layer with a number of neurons which is only a fraction of the number of input features.
\section{Problem 4: Gradient Descent}
