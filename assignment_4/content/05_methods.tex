\chapter{Methods}
\section{Problem 1: Simple network}
The task of problem 1 is to calculate the output of the given network using matrix operations.
In order to execute the operations firstly the operation should be defined.
It can be formulated as $x_j=f(x_{i}*w_{ij})$ with $x_j$ being the output of layer $j$, $x_{i}$ being the output of layer $i$, $w_{ij}$ being the weight matrix for the connections between the layers $i$ and $j$ and $f()$ being the activation function of layer $j$ which in our case is the sigmoid function.\\

   
\section{Problem 2: Backpropagation}
The task of problem 1 is to calculate the back propagation of the given network (section 1.2) for two different cost functions. This is done by first finding the gradient using the partial derivatives of the forward propagation function, taking advantage of the chain rule this can be done layer by layer from end to beginning. This is use to determine the influence of each single weight and bias to the final error, which are updated accordingly to minimize the error.
\section{Problem 3: Artificial neural network}

In problem 3 both of the tasks are combined and put to use for the training of a simple neural network with the help of the Pytorch library. The best suiting combination of network architecture and hyperparameters have to be found empirically by making assumptions and testing them by comparing the results of the different tests.
\section{Problem 4: Gradient Descent}\label{ch:methods:sec:4}

There are multiple versions of the gradient descent.
They are based on the normal gradient descent.

\subsection{Normal Gradient Descent}

Gradient Descent is one optimizer to find better weights in the model.
It is based on the loss function $J$ which represents the classification error and the gradients to give the direction to minimize the loss.
These gradients are calculated by back propagation.
There is also the learning rate $\lambda$ which controls the size of each step in one direction.
With a low learning rate comes high computation times but a to high learning rate can lead to a miss calculation of the minimum.

The normal gradient descent is based on the complete training set and has hight costs with big data sets.
The height costs are effected by only taking a single step for one pass.

In short the algorithm of the normal gradient descent is as follows:

\begin{itemize}
	\item For each epoch:
	\begin{itemize}
		\item For each weight $j$:
		\begin{itemize}
            \item $w_j = w_{j-1} + \delta{w_j}$
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsection{Stochastic Gradient Descent}

This can be optimized by using the \ac{SGD} where the weights are updated after each training sample.
The stochastic approximation of the true cost gradient results in a zig-zag path.
This only works fine with a convex cost function.

The extension provided by \ac{SGD} leads to the following algorithm:

\begin{itemize}
	\item For each epoch:
	\begin{itemize}
        \item{For each training sample}
            \begin{itemize}
            \item For each weight $j$:
            \begin{itemize}
                \item $w_j = w_{j-1} + \delta{w_j}$
            \end{itemize}
        \end{itemize}
	\end{itemize}
\end{itemize}

\subsection{Gradient Descent with momentum}

The idea behind this extension is to move only for the average of the gradients.
This prevent a possible oscillation and speeds up the optimization significantly.

In comparison to the normal gradient descent ($w_t = w_{t-1} - \alpha g_{t-1}$) the weight looks like $w_t = w_{t-1} - \alpha v_t$ with $v_t = \beta v_{t-1} - (1 -\beta)g_{t-1}$ and $\beta = 0.9$.

\subsection{RMSProp}

Another idea is to make the step size inversively proportional to the magnitude of the gradient which is called the \ac{RMSProp}.
Doing this it is necessary to introduce a new variable $s$ which is the moving average of squared gradients.

\begin{equation}\label{RMS1}
    s_t = \beta s_{t-1} + (1 - \beta)g^2_{t-1}
\end{equation}

With the weight:

\begin{equation}\label{RMS2}
    w_t = w_{t-1} - \alpha \frac{g_{t-1}}{\sqrt{s_t}+\varepsilon}
\end{equation}

\subsection{Adaptive moment estimation}

The combination of \ac{RMSProp} and Gradient descent with momentum is called \ac{ADAM}.

The equations are:


\begin{equation}\label{ADAM1}
    v_t = \frac{\beta_1 v_{t-1} - (1 - \beta_1)g_{t-1}}{1 - \beta^t_1}
\end{equation}

\begin{equation}\label{ADAM2}
    s_t = \frac{\beta_2 s_{t-1} + (1 - \beta_2)g^2_{t-1}}{1 - \beta^t_2}
\end{equation}

\begin{equation}\label{RMS2}
    w_t = w_{t-1} - \alpha \frac{v_t}{\sqrt{s_t}+\varepsilon}
\end{equation}

With $\beta_1 = 0.9; \beta_2 = 0.999; \varepsilon = 10^{-8}; \alpha $ to be tuned.

Since the data points should all satisfy the equation $y = mx + c$ in this assignment the gradients for $m$ and $c$ are calculated with the following equations.

\begin{equation}\label{gradients1}
    \frac{\partial l}{\partial m} = -2 \sum\limits_{i=1}^n x_i (y_i - mx_i - c)
\end{equation}

\begin{equation}\label{gradients2}
    \frac{\partial l}{\partial c} = -2 \sum\limits_{i=1}^n (y_i - mx_i - c)
\end{equation}

Where the residuals are calculated with

\begin{equation}\label{gradients2}
    e_i = y_i - mx_i - c
\end{equation}

And the loss is calculated with

\begin{equation}\label{gradients2}
    m_k = m_{k-1} - \lambda \frac{\partial l}{\partial m}
\end{equation}

\begin{equation}\label{gradients2}
    c_k = c_{k-1} - \lambda \frac{\partial l}{\partial c}
\end{equation}

With $\lambda$ used as learning parameter.



